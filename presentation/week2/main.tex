\documentclass[9pt]{beamer}
\usepackage[utf8]{inputenc}

% \usepackage{animate}

%Information to be included in the title page:
\title{Probabilitic Numerical Methods for Evaluating the Max Integral Operator}
\author{Nishad Gothoskar}
\institute{Learning and Intelligent Systems}
\date{July 27 2018}
 
\begin{document}
 
\frame{\titlepage}
 
\begin{frame}
\frametitle{Max Integral Operator}
In this work, we will explore techniques of estimating expressions of the following form:
\begin{equation*}
    \max_{a} \int f(a,s) \ \text{d}s
\end{equation*}
In high dimensions and continuous spaces, exact evaluation is many times intractable. As a result we have to take a probabilistic numerical approach to accurately and efficiently estimate its value.
\newline \newline
The Max Integral Operator is used in a variety of computational problems across many fields. In 
Reinforcement Learning it is used in MDPs for value update and POMDPs for belief updates. In inference problems we may use in computing the maximum a posteriori estimate. Intuitively, maximizing over some parameters while integrating out others is a useful/important operation.
\end{frame}

\begin{frame}
\frametitle{Bayesian Quadrature}
Quadrature techniques, first presented as Bayes-Hermite Quadrature [1], are used to actively sample function evaluations in order to reduce uncertainty about an integral's value. This is usually done by maintaining a Gaussian Process over the integrand and then integrating the GP. 
\newline\newline
Most work in BQ has focused on integrals of the form:
\begin{equation*}
   \int f(x) p(x)  \ \text{d}x
\end{equation*}
In our framework, we will use this distribution over the integral's value as a convergence criterion, which can guide the optimization (maximization) procedure.
\end{frame}

% \begin{frame}
% \frametitle{Monte Carlo}
% But why not use Monte Carlo to sample from $p$ and estimate the integral?
% \newline\newline
% Most work in BQ has focused on integrals of the form:
% \begin{equation*}
%    \int f(x) p(x)  \ \text{d}x
% \end{equation*}
% In our framework, we will use this distribution over the integral's value as a convergence criterion, which can guide the optimization (maximization) procedure.
% \end{frame}

\begin{frame}
\frametitle{Basic Max Integral Optimization}
We will first demonstrate our method on a simple 2 dimensional function:
\begin{equation*}
\begin{split}
   f(s,a) &= \mathcal{N}(s | 3.0, 1.0) * \mathcal{N}(a | 0.0, 1.0) \\
   p(s) &= \mathcal{N}(s | 0.0, 10.0) \\
\end{split}
\end{equation*}
Now we will estimate:
\begin{equation*}
\begin{split}
   \max_{a} \int f(a,s) p(s) \ \text{d}s
\end{split}
\end{equation*}
\end{frame}

\begin{frame}
\frametitle{Basic Max Integral Optimization}
For this scenario we will discretize the action space and use a few intial points to get a rough estimate of the integral. Then we will proceed in a multi armed bandits fashion according to an acquisition function. Here, we use the UCB acquisition function. After we select the next query point $a^*$, we make queries to $f(\cdot , a^*)$ and further refine our estimate of the integral that action.

We repeat the above procedure until a probabilistic convergence criterion is reached (action $a^*$ is the argmax with probability $p$). Below is an animation showing execution of the optimization.
\end{frame}

\begin{frame}
\frametitle{Efficient Optimization}
In higher dimensions and larger spaces, quadrature (even in settings where there is a closed form solution) can be an expensive operation. By accounting for the distribution over value, we can focus on evaluating actions with potential of high value.
\end{frame}

\begin{frame}
\frametitle{Max Integral in RL}
In Reinforcement Learning, the Bellman Update (in continuous state space) is:

\[V^{\pi}(s) = \max_{a \in A} \int_{s' \in S} p(s'|s,a)\big(R(s'|s,a) + \gamma^{\Delta t}V^{\pi}(s')\big) \ \textup{d}s' \]

If we use a GP to model the Value Function, then we need to maximize over actions. It is infeasible to fully compute this integral for each action.
\end{frame}



\begin{frame}
\frametitle{Bibliography}
[1] O’Hagan, A. (1991). Bayes–Hermite quadrature. Journal of Statistical Planning and Inference, 29(3), 245–260.\newline
[2]   M.  P.  Deisenroth,  J.  Peters,  and  C.  E.  Rasmussen,  “Approximate dynamic programming with Gaussian processes,” in Proc. of the IEEE American Control Conference (ACC), 2008, pp. 4480–4485.\newline
[3] Rasmussen, C.E., Ghahramani, Z.: Bayesian Monte Carlo. In Becker, S., Obermayer, K., eds.: Advances in Neural Information Processing Systems. Volume 15. MIT Press, Cambridge, MA (2003)
\end{frame}

 
\end{document}

